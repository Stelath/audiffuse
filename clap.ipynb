{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/korte/micromamba/envs/ml/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-09-03 21:16:51,216] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import torch\n",
    "import laion_clap\n",
    "from laion_clap.training.data import get_audio_features\n",
    "from es_dataset import EpidemicSoundDataset, EpidemicSoundDataModule\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantization\n",
    "def int16_to_float32(x):\n",
    "    return (x / 32767.0).astype(np.float32)\n",
    "\n",
    "\n",
    "def float32_to_int16(x):\n",
    "    x = np.clip(x, a_min=-1., a_max=1.)\n",
    "    return (x * 32767.).astype(np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = laion_clap.CLAP_Module(enable_fusion=True)\n",
    "model.load_ckpt(model_id=3) # download the default pretrained checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.model.audio_branch.state_dict(), '/scratch/korte/audiffuse/clap.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLAPAudioCfp(model_type='HTSAT', model_name='tiny', sample_rate=48000, audio_length=1024, window_size=1024, hop_size=480, fmin=50, fmax=14000, class_num=527, mel_bins=64, clip_samples=480000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.audio_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_waveform = int16_to_float32(float32_to_int16(audio_data))\n",
    "audio_waveform = torch.from_numpy(audio_waveform).float()\n",
    "\n",
    "temp_dict = {}\n",
    "temp_dict = get_audio_features(\n",
    "    temp_dict, audio_waveform, 480000, \n",
    "    data_truncating='fusion', \n",
    "    data_filling='repeatpad',\n",
    "    audio_cfg=model.model_cfg['audio_cfg'],\n",
    "    require_grad=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_dict['mel_fusion'].requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio_length': 1024,\n",
       " 'clip_samples': 480000,\n",
       " 'mel_bins': 64,\n",
       " 'sample_rate': 48000,\n",
       " 'window_size': 1024,\n",
       " 'hop_size': 480,\n",
       " 'fmin': 50,\n",
       " 'fmax': 14000,\n",
       " 'class_num': 527,\n",
       " 'model_type': 'HTSAT',\n",
       " 'model_name': 'tiny'}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model_cfg['audio_cfg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get audio embeddings from audio data\n",
    "audio_data, _ = librosa.load('test_song.mp3', sr=48000) # sample rate should be 48000\n",
    "# audio_data = audio_data[:48000*15] # 5 seconds\n",
    "audio_data = audio_data.reshape(1, -1) # Make it (1,T) or (N,T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# es_dataset = EpidemicSoundDataset('/fastscratch/korte/es_dataset/')\n",
    "es_dataset = EpidemicSoundDataModule('/fastscratch/korte/es_dataset/')\n",
    "es_dataset.setup(None)\n",
    "loader = es_dataset.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/korte/micromamba/envs/ml/lib/python3.9/site-packages/torchaudio/transforms/_transforms.py:611: UserWarning: Argument 'onesided' has been deprecated and has no influence on the behavior of this module.\n",
      "  warnings.warn(\n",
      "/home/korte/micromamba/envs/ml/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "audio_set = next(iter(loader))['audio']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['mel_fusion', 'longer', 'waveform'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_set.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.bool"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_set['longer'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.7 ms ± 1.28 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "audio_embed = model.get_audio_embedding_from_data(x = audio_data, use_tensor=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.02798386  0.09505272 -0.03748165 -0.03948206  0.03360447 -0.03106373\n",
      "   0.04163197 -0.00567254  0.0260455   0.02825583  0.04025021  0.02217655\n",
      "  -0.01997084 -0.01692094 -0.0323186   0.01411903  0.01598474  0.05030755\n",
      "  -0.01884449 -0.05638063]]\n",
      "(1, 512)\n"
     ]
    }
   ],
   "source": [
    "print(audio_embed[:,-20:])\n",
    "print(audio_embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0131,  0.0785,  0.0767, -0.0259, -0.0184,  0.0028,  0.0451,  0.0133,\n",
      "         -0.0327,  0.0620, -0.0639,  0.0697,  0.0027, -0.0418, -0.0539,  0.0003,\n",
      "         -0.0098, -0.0034, -0.0337, -0.0032]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "# Get audio embeddings from audio data\n",
    "audio_data, _ = librosa.load('test_song.mp3', sr=48000) # sample rate should be 48000\n",
    "audio_data = audio_data.reshape(1, -1) # Make it (1,T) or (N,T)\n",
    "audio_data = torch.from_numpy(int16_to_float32(float32_to_int16(audio_data))).float() # quantize before send it in to the model\n",
    "audio_embed = model.get_audio_embedding_from_data(x = audio_data, use_tensor=True)\n",
    "print(audio_embed[:,-20:])\n",
    "print(audio_embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "audio_data = es_dataset[0]['audio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio_embed = model.model.get_audio_embedding([audio_data])\n",
    "audio_embeds = model.model.encode_audio(audio_set, device=torch.device('cuda'))[\"fine_grained_embedding\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1024, 768])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 256, 768])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.avg_pool1d(audio_embeds.permute(0, 2, 1), kernel_size=4, padding=1).permute(0, 2, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "projection = model.model.audio_projection(audio_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1024, 512])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projection.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_embed.detach()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
