{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/korte/micromamba/envs/ml/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "from transformers import AutoModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: feature_extractor_cqt requires the libray 'nnAudio'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at m-a-p/MERT-v1-95M were not used when initializing MERTModel: ['encoder.pos_conv_embed.conv.weight_v', 'encoder.pos_conv_embed.conv.weight_g']\n",
      "- This IS expected if you are initializing MERTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MERTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MERTModel were not initialized from the model checkpoint at m-a-p/MERT-v1-95M and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# loading our model weights\n",
    "model = AutoModel.from_pretrained(\"m-a-p/MERT-v1-95M\", trust_remote_code=True)\n",
    "model = model.to(\"cuda\")\n",
    "\n",
    "# loading the corresponding preprocessor config\n",
    "processor = Wav2Vec2FeatureExtractor.from_pretrained(\"m-a-p/MERT-v1-95M\",trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8641\n",
      "setting rate from 44100 to 24000\n",
      "6960\n",
      "setting rate from 44100 to 24000\n"
     ]
    }
   ],
   "source": [
    "# load audio files\n",
    "audio_files = ['rockin_around.mp3', 'test_song.mp3']\n",
    "input_audio = []\n",
    "\n",
    "for audio_file in audio_files:\n",
    "    # audio, sampling_rate = librosa.load(audio_file, sr=None)\n",
    "    audio, sampling_rate = torchaudio.load(audio_file, backend='ffmpeg')\n",
    "    audio = audio[0] # only one channel\n",
    "    \n",
    "    first_nonzero_index = next((index for index, value in enumerate(audio) if value != 0), None)\n",
    "    print(first_nonzero_index)\n",
    "\n",
    "    resample_rate = processor.sampling_rate\n",
    "    # make sure the sample_rate aligned\n",
    "    if resample_rate != sampling_rate:\n",
    "        print(f'setting rate from {sampling_rate} to {resample_rate}')\n",
    "        resampler = T.Resample(sampling_rate, resample_rate)\n",
    "    else:\n",
    "        resampler = None\n",
    "\n",
    "    # audio file is decoded on the fly\n",
    "    if resampler is not None:\n",
    "        audio = resampler(audio)\n",
    "    \n",
    "    input_audio.append(audio.numpy())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3021845,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_audio[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(input_audio, sampling_rate=resample_rate, return_tensors=\"pt\", padding=True)\n",
    "inputs = inputs.to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_values', 'attention_mask'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 2, 9443, 768])\n",
      "torch.Size([13, 2, 768])\n"
     ]
    }
   ],
   "source": [
    "# take a look at the output shape, there are 13 layers of representation\n",
    "# each layer performs differently in different downstream tasks, you should choose empirically\n",
    "all_layer_hidden_states = torch.stack(outputs.hidden_states).squeeze()\n",
    "print(all_layer_hidden_states.shape) # [13 layer, Time steps, 768 feature_dim]\n",
    "\n",
    "# for utterance level classification tasks, you can simply reduce the representation in time\n",
    "time_reduced_hidden_states = all_layer_hidden_states.mean(-2)\n",
    "print(time_reduced_hidden_states.shape) # [13, 768]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 4 is not equal to len(dims) = 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/korte/projects/audifuse/wav2vec2.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://tunnel%2Bpalmetto/home/korte/projects/audifuse/wav2vec2.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m audio_embeds \u001b[39m=\u001b[39m all_layer_hidden_states\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://tunnel%2Bpalmetto/home/korte/projects/audifuse/wav2vec2.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m audio_embeds_avg_pool \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mavg_pool1d(audio_embeds\u001b[39m.\u001b[39;49mpermute(\u001b[39m0\u001b[39;49m, \u001b[39m2\u001b[39;49m, \u001b[39m1\u001b[39;49m), kernel_size\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://tunnel%2Bpalmetto/home/korte/projects/audifuse/wav2vec2.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m audio_embeds_max_pool \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmax_pool1d(audio_embeds\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m), kernel_size\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 4 is not equal to len(dims) = 3"
     ]
    }
   ],
   "source": [
    "audio_embeds = all_layer_hidden_states.permute(1, 0, 2, 3)\n",
    "audio_embeds_avg_pool = F.avg_pool1d(audio_embeds.permute(0, 2, 1), kernel_size=4, padding=1).permute(0, 2, 1)\n",
    "audio_embeds_max_pool = F.max_pool1d(audio_embeds.permute(0, 2, 1), kernel_size=4, padding=1).permute(0, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trunc = time_reduced_hidden_states.permute(1,0,2)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_trunc = time_reduced_hidden_states.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/korte/projects/audifuse/wav2vec2.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://tunnel%2Bpalmetto/home/korte/projects/audifuse/wav2vec2.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m F\u001b[39m.\u001b[39;49mmse_loss(trunc, non_trunc)\n",
      "File \u001b[0;32m/zfs/ailab/Tools/micromamba/envs/ml/lib/python3.9/site-packages/torch/nn/functional.py:3318\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3314\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, target):\n\u001b[1;32m   3315\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   3316\u001b[0m         mse_loss, (\u001b[39minput\u001b[39m, target), \u001b[39minput\u001b[39m, target, size_average\u001b[39m=\u001b[39msize_average, reduce\u001b[39m=\u001b[39mreduce, reduction\u001b[39m=\u001b[39mreduction\n\u001b[1;32m   3317\u001b[0m     )\n\u001b[0;32m-> 3318\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (target\u001b[39m.\u001b[39;49msize() \u001b[39m==\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize()):\n\u001b[1;32m   3319\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   3320\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUsing a target size (\u001b[39m\u001b[39m{\u001b[39;00mtarget\u001b[39m.\u001b[39msize()\u001b[39m}\u001b[39;00m\u001b[39m) that is different to the input size (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize()\u001b[39m}\u001b[39;00m\u001b[39m). \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3321\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThis will likely lead to incorrect results due to broadcasting. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3322\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease ensure they have the same size.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   3323\u001b[0m         stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m   3324\u001b[0m     )\n\u001b[1;32m   3325\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "F.mse_loss(trunc, non_trunc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
