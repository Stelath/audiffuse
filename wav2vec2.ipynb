{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/zfs/ailab/Tools/micromamba/envs/ml/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "from transformers import AutoModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: feature_extractor_cqt requires the libray 'nnAudio'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at m-a-p/MERT-v1-95M were not used when initializing MERTModel: ['encoder.pos_conv_embed.conv.weight_g', 'encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing MERTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MERTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MERTModel were not initialized from the model checkpoint at m-a-p/MERT-v1-95M and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of MERTModel were not initialized from the model checkpoint at m-a-p/MERT-v1-95M and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# loading our model weights\n",
    "model = AutoModel.from_pretrained(\"m-a-p/MERT-v1-95M\", trust_remote_code=True)\n",
    "model = model.to(\"cuda\")\n",
    "\n",
    "# loading the corresponding preprocessor config\n",
    "processor = Wav2Vec2FeatureExtractor.from_pretrained(\"m-a-p/MERT-v1-95M\",trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8641\n",
      "setting rate from 44100 to 24000\n",
      "6960\n",
      "setting rate from 44100 to 24000\n"
     ]
    }
   ],
   "source": [
    "# load audio files\n",
    "audio_files = ['rockin_around.mp3', 'test_song.mp3']\n",
    "input_audio = []\n",
    "\n",
    "for audio_file in audio_files:\n",
    "    # audio, sampling_rate = librosa.load(audio_file, sr=None)\n",
    "    audio, sampling_rate = torchaudio.load(audio_file, backend='ffmpeg')\n",
    "    audio = audio[0] # only one channel\n",
    "    \n",
    "    first_nonzero_index = next((index for index, value in enumerate(audio) if value != 0), None)\n",
    "    print(first_nonzero_index)\n",
    "\n",
    "    resample_rate = processor.sampling_rate\n",
    "    # make sure the sample_rate aligned\n",
    "    if resample_rate != sampling_rate:\n",
    "        print(f'setting rate from {sampling_rate} to {resample_rate}')\n",
    "        resampler = T.Resample(sampling_rate, resample_rate)\n",
    "    else:\n",
    "        resampler = None\n",
    "\n",
    "    # audio file is decoded on the fly\n",
    "    if resampler is not None:\n",
    "        audio = resampler(audio)\n",
    "    \n",
    "    input_audio.append(audio.numpy())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3021845,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_audio[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(input_audio, sampling_rate=resample_rate, return_tensors=\"pt\", padding=True)\n",
    "inputs = inputs.to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_values', 'attention_mask'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 9022, 768])\n",
      "torch.Size([13, 768])\n"
     ]
    }
   ],
   "source": [
    "# take a look at the output shape, there are 13 layers of representation\n",
    "# each layer performs differently in different downstream tasks, you should choose empirically\n",
    "all_layer_hidden_states = torch.stack(outputs.hidden_states).squeeze()\n",
    "print(all_layer_hidden_states.shape) # [13 layer, Time steps, 768 feature_dim]\n",
    "\n",
    "# for utterance level classification tasks, you can simply reduce the representation in time\n",
    "time_reduced_hidden_states = all_layer_hidden_states.mean(-2)\n",
    "print(time_reduced_hidden_states.shape) # [13, 768]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trunc = time_reduced_hidden_states.permute(1,0,2)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_trunc = time_reduced_hidden_states.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1839,  0.0560,  0.3645,  ..., -0.1801, -0.2498,  0.0766],\n",
       "        [-0.1308, -0.1409,  0.5574,  ..., -0.2917, -0.0786,  0.0187],\n",
       "        [ 0.0600, -0.0027,  0.3828,  ..., -0.0912, -0.0442, -0.0882],\n",
       "        ...,\n",
       "        [-0.0450, -0.1034,  0.0135,  ..., -0.1094, -0.0755, -0.0220],\n",
       "        [ 0.0359, -0.0826, -0.0972,  ..., -0.1007, -0.0177,  0.0244],\n",
       "        [-0.0394, -0.0164, -0.0542,  ...,  0.1010, -0.1002,  0.0485]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.18856402,  0.05180721,  0.4061588 , ..., -0.1926618 ,\n",
       "        -0.25924715,  0.07684468],\n",
       "       [-0.13712476, -0.13524362,  0.60284626, ..., -0.30100065,\n",
       "        -0.07869767,  0.0167464 ],\n",
       "       [ 0.05161031,  0.00725114,  0.42961738, ..., -0.09042618,\n",
       "        -0.03980322, -0.09716687],\n",
       "       ...,\n",
       "       [-0.0262141 , -0.09254253,  0.02330886, ..., -0.12008666,\n",
       "        -0.06740732, -0.01618211],\n",
       "       [ 0.0374712 , -0.0803475 , -0.08498251, ..., -0.1117202 ,\n",
       "        -0.01279834,  0.03500533],\n",
       "       [-0.04584039, -0.01758612, -0.03769425, ...,  0.09866826,\n",
       "        -0.10444974,  0.05512224]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_trunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/korte/projects/audifuse/wav2vec2.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://tunnel%2Bpalmetto/home/korte/projects/audifuse/wav2vec2.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m F\u001b[39m.\u001b[39;49mmse_loss(trunc, non_trunc)\n",
      "File \u001b[0;32m/zfs/ailab/Tools/micromamba/envs/ml/lib/python3.9/site-packages/torch/nn/functional.py:3318\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3314\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, target):\n\u001b[1;32m   3315\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   3316\u001b[0m         mse_loss, (\u001b[39minput\u001b[39m, target), \u001b[39minput\u001b[39m, target, size_average\u001b[39m=\u001b[39msize_average, reduce\u001b[39m=\u001b[39mreduce, reduction\u001b[39m=\u001b[39mreduction\n\u001b[1;32m   3317\u001b[0m     )\n\u001b[0;32m-> 3318\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (target\u001b[39m.\u001b[39;49msize() \u001b[39m==\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize()):\n\u001b[1;32m   3319\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   3320\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUsing a target size (\u001b[39m\u001b[39m{\u001b[39;00mtarget\u001b[39m.\u001b[39msize()\u001b[39m}\u001b[39;00m\u001b[39m) that is different to the input size (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize()\u001b[39m}\u001b[39;00m\u001b[39m). \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3321\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThis will likely lead to incorrect results due to broadcasting. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3322\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease ensure they have the same size.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   3323\u001b[0m         stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m   3324\u001b[0m     )\n\u001b[1;32m   3325\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "F.mse_loss(trunc, non_trunc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
